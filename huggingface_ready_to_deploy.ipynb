{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be0fbb8-5dd1-467b-b0de-257c25acd23a",
   "metadata": {},
   "source": [
    "### Local test with hugging face Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4cab390-37ec-47ce-aafe-4fcf9f3bee15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c83b32-0236-450a-90da-5c208031fa2f",
   "metadata": {},
   "source": [
    "#### HuggingFace FLAN-T5\n",
    "https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/flan-t5#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7260db-0bdc-4eac-8c56-db95e66fca70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cc922-bdbd-422b-b0c1-53c60480ca41",
   "metadata": {},
   "source": [
    "#### Task: text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7dbd9-dca4-4ade-b3ae-32f90808d991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf0939-4735-4f39-a87a-46094f43107f",
   "metadata": {},
   "source": [
    "#### Task: translation English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16023138-56f0-4716-bc76-65ba65212173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to Francais: The house is wonderful.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0ba85-94ad-424b-ba37-ff4c8901f0c5",
   "metadata": {},
   "source": [
    "#### Task: translation English to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94f1b8-5fe0-48a0-a3ac-7694a7368104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a55dca-aa44-471f-ae74-3a87af2a1df9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task: sentiment classification positive negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c33620-d752-4bc3-aef7-b35a2e235bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer('Sentiment classification: The house is wonderful', return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56852f-5aeb-41b1-acb7-0c8c9efce17d",
   "metadata": {},
   "source": [
    "### Deploy Flan T5 on SageMaker and test with hosting services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34926180-58a8-4a9c-9d7e-ba6a9ff2864e",
   "metadata": {},
   "source": [
    "Here is a nice blog to explain https://www.philschmid.de/deploy-flan-t5-sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a21ecd2c-c8f7-43d9-8b6a-347c6a52a160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::707684582322:role/service-role/AmazonSageMaker-ExecutionRole-20191024T163188\n",
      "sagemaker bucket: sagemaker-eu-west-1-707684582322\n",
      "sagemaker session region: eu-west-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cdebb12-e6e7-41a7-8d20-c35a52377e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00796198844909668,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 10 files",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9a38e57589417f9102541eff358163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01080465316772461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 3132668804,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf78cf0844924f35926b4400a78b454d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011083841323852539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading spiece.model",
       "rate": null,
       "total": 791656,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb72e161aa3b4771a0dd256afa1d71d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024441003799438477,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 3132781861,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21b7f010f514b94ab3826025bade68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.038248538970947266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 2201,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c0163857ec4db59dbcdc25463048e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03353691101074219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)de16a2135a/README.md",
       "rate": null,
       "total": 10767,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7464114af974bb3a9ffe16e596eac6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)de16a2135a/README.md:   0%|          | 0.00/10.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03185677528381348,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)2135a/.gitattributes",
       "rate": null,
       "total": 1434,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b519fd5cb4b42e1ad0757fe629d1e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2135a/.gitattributes:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03544759750366211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)16a2135a/config.json",
       "rate": null,
       "total": 662,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9440b7a6f0d444f59598ce90b5bfa4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)16a2135a/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03732562065124512,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 147,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18dbe19ee66842de84cea057ac48d9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00795292854309082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)2135a/tokenizer.json",
       "rate": null,
       "total": 2424064,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8c6df4a66348a4afe6a101b5eee571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)2135a/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00803685188293457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 2539,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03465bc1b2cf40889482a959cf1e17cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID=\"google/flan-t5-large\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "model_tar_dir.mkdir()\n",
    "\n",
    "# setup temporary directory\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    # download snapshot\n",
    "    snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID, cache_dir=tmpdir,ignore_patterns=[\"*.msgpack\", \"*.h5\"])\n",
    "    # copy snapshot to model dir\n",
    "    copy_tree(snapshot_dir, str(model_tar_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "580becd3-2f88-40c3-8661-9194a1fdfda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from distutils.file_util import copy_file\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from os import path\n",
    "\n",
    "\n",
    "# copy scripts/ to model dir, this step is not neccessary with SageMaker HuggingFace\n",
    "HF_MODEL_ID=\"google/flan-t5-large\"\n",
    "# create model dir\n",
    "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
    "if not path.exists(model_tar_dir):\n",
    "    model_tar_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e286b738-7d49-4bab-bf83-00401471f896",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtyping\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dict, List, Any\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSeq2SeqLM, AutoTokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# load model and processor from model_dir\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model =  AutoModelForSeq2SeqLM.from_pretrained(model_dir)\u001b[37m\u001b[39;49;00m\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\u001b[37m\u001b[39;49;00m\n",
      "    model.eval()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device), tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(data, model_and_tokenizer):\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# unpack model and tokenizer\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    model, tokenizer = model_and_tokenizer\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# process input\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    inputs = data.pop(\u001b[33m\"\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, data)\u001b[37m\u001b[39;49;00m\n",
      "    parameters = data.pop(\u001b[33m\"\u001b[39;49;00m\u001b[33mparameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mNone\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# preprocess\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    input_ids = tokenizer(inputs, return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).input_ids\u001b[37m\u001b[39;49;00m\n",
      "    input_ids = input_ids.to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# pass inputs with all kwargs in data\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m parameters \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        outputs = model.generate(input_ids, **parameters)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        outputs = model.generate(input_ids)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# postprocess the prediction\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    prediction = tokenizer.decode(outputs[\u001b[34m0\u001b[39;49;00m], skip_special_tokens=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m [{\u001b[33m\"\u001b[39;49;00m\u001b[33mgenerated_text\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: prediction}]\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/inference_flan_t5_model_hub.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "098d1602-2b93-4dac-b2c7-01a51a6136f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load model and processor from model_dir\n",
    "    model =  AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    return model.to(device), tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # unpack model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # process input\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # preprocess\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        outputs = model.generate(input_ids, **parameters)\n",
    "    else:\n",
    "        outputs = model.generate(input_ids)\n",
    "\n",
    "    # postprocess the prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return [{\"generated_text\": prediction}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834587d-5de9-493e-bb77-4d7cce663626",
   "metadata": {},
   "source": [
    "#### Local test the inference code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cb964fa-f1a8-4f5d-93a1-fe7361a68acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Sentiment classification: The house is wonderful\"\n",
    "input_json = {\n",
    "    \"inputs\": text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "641f48c3-99b6-412e-9f5e-cc903983aed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_and_tokenizer = model_fn(\"./flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c159c42c-3bed-470e-8cc1-6ae5d120f3e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = predict_fn(input_json,model_and_tokenizer)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f6435ac-3b8c-475f-a225-187b168e3ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('flan-t5-large/code/inference.py', 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_code_dir = Path(HF_MODEL_ID.split(\"/\")[-1] + \"/code\")\n",
    "if not path.exists(inference_code_dir):\n",
    "    inference_code_dir.mkdir()\n",
    "copy_file(src=\"./scripts/inference_flan_t5_model_hub.py\",dst=path.join(str(inference_code_dir),\"inference.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "776c0b9d-449e-4bc7-90ef-48c0cc646885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spiece.model\n",
      "special_tokens_map.json\n",
      "model.safetensors\n",
      "README.md\n",
      "tokenizer_config.json\n",
      ".gitattributes\n",
      "config.json\n",
      "tokenizer.json\n",
      "code\n",
      "pytorch_model.bin\n",
      "generation_config.json\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b1fa9cd-2cc4-4308-b47e-43c9cb4dd5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model uploaded to: s3://sagemaker-eu-west-1-707684582322/flan-t5-large/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/flan-t5-large\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14116ccb-804c-4b40-9f83-dedfdf966a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_model_uri = \"s3://sagemaker-eu-west-1-707684582322/flan-t5-large/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92419feb-7e5d-4c74-bc63-2d4b0bc6d245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837e0b2-8654-4595-8e70-6c89ec2008c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7bee0a-f51e-4370-9872-8e18d9054940",
   "metadata": {},
   "source": [
    "#### Run inference using the deployed model with python sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "283404b2-95be-4790-80c0-4a17f54a74f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ebaf843-6733-4b2f-98d6-c61cf62932f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"sagemaker-runtime\")\n",
    "#Put the correct endpoint name \n",
    "#endpoint_name = \"huggingface-pytorch-inference-2023-03-24-03-24-49-183\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2250333-d2b9-453e-bce3-3bc07ade9be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Sentiment classification: The house is wonderful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d20537f-6cc9-4b7f-9bbb-04eba515acba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_json = {\n",
    "    \"inputs\": text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35dde1e1-5b3c-40fe-a954-89e94b7f3c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[{\"generated_text\":\"positive\"}]'\n"
     ]
    }
   ],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(input_json),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec0014-f825-43c1-8dd3-a4edde3a9fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
