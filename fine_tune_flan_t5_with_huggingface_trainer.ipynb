{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc691ebc-6825-4fdc-97bd-198cf42de45c",
   "metadata": {},
   "source": [
    "Fine-tune FLAN-T5 for chat & dialogue summarization\n",
    "https://www.philschmid.de/fine-tune-flan-t5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8782798-7bd9-4721-a716-55696be72589",
   "metadata": {},
   "source": [
    "#### Load the dataset from huggingface and save to local disk for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633db3ae-2e26-4a8b-b083-597811fdbeca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install py7zr --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfef14-2800-4217-bf2d-2bb09466c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "dataset_id = \"samsum\"\n",
    "# Load dataset from the hub\n",
    "dataset_hf = load_dataset(dataset_id)\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset_hf['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset_hf['test'])}\")\n",
    "\n",
    "# Train dataset size: 14732\n",
    "# Test dataset size: 819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8c22e-1a0f-41be-aef3-4451f5e2f212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the dataset to local disk for future use\n",
    "dataset_hf.save_to_disk('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a73f5-080b-40be-8dab-4baa1dc751d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load the dataset from local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d12a45-f5f9-4830-b5d8-86687c638557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_from_disk('./dataset')\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1813e-ceb1-449e-a632-0e84d3372c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = dataset['test'][2]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "print(f\"summary: \\n{sample['summary']}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e99b6a-e862-4b5b-b6fb-8dbf36c5af56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample['dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1a606-f5dd-4c6c-955e-807fe245d381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_id=\"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928bd6b-7cd6-4f26-ae7f-a050565c4d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6747ab-e3ae-4904-8fdc-55a6ada63995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8df9a-a4d9-4362-9dfe-3ee36e0fadc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7d5c1-2fb8-4a1c-a8fb-5da34f014181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3df5d5-f1a3-4e49-85be-37a620dc05a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163ef27-8198-4f02-83b0-95deb0553373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_inputs = tokenizer(\"This is a test\", max_length=max_source_length, padding=\"max_length\", truncation=True)\n",
    "#model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf0b90-fd81-4d4b-ab65-d0800ebdcf6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6a530-605c-4bff-b2c4-dd9613730fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str(tokenized_dataset['train'][2]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcba18c-3385-4d6b-8cd1-f7b71ca26ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id=\"google/flan-t5-small\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23569057-91b2-4aff-8566-8e30bacbeff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2bf7d-7bcb-4693-9fca-bec7b153efd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-{dataset_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2899de-ea1a-4366-bfff-6c7cfb7cccef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # Overflows with fp16\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"overall_f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a949f10-d63a-48c9-9248-2e3fd3e85c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c91ff0-3f72-4f8d-b435-e7311e443943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5507e-330a-4db1-b9ce-7133a36393ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/train_flan_t5_sm_compatible.py --epochs 1 --training_dir \"./dataset\" --output_dir \"./model\" --train_batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b19698-43e3-4792-85f7-663d136bb8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
