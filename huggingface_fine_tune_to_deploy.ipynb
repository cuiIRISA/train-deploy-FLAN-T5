{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cab390-37ec-47ce-aafe-4fcf9f3bee15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece\n",
    "!pip install -U -q sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56852f-5aeb-41b1-acb7-0c8c9efce17d",
   "metadata": {},
   "source": [
    "### Deploy Flan T5 on SageMaker and test with hosting services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34926180-58a8-4a9c-9d7e-ba6a9ff2864e",
   "metadata": {},
   "source": [
    "Here is a nice blog to explain https://www.philschmid.de/deploy-flan-t5-sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ecd2c-c8f7-43d9-8b6a-347c6a52a160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40af97-b644-4b5b-bf3b-bd70c6f2ee4b",
   "metadata": {},
   "source": [
    "#### We can download the tokenizer from huggingface repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffde23-b9bc-4647-9e01-3fdecb23daa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID=\"google/flan-t5-small\"\n",
    "# create model dir\n",
    "model_tar_dir = Path('fine-tuned-' + HF_MODEL_ID.split(\"/\")[-1])\n",
    "\n",
    "# setup temporary directory\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    # download snapshot\n",
    "    snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID, cache_dir=tmpdir,ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*model*\"])\n",
    "    # copy snapshot to model dir\n",
    "    copy_tree(snapshot_dir, str(model_tar_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87505730-d3d4-45fe-b442-705a05d9fef5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PYTORCH_MODEL_LOCATION = \"./model\"\n",
    "MDDEL_TAR_NAME = \"model.tar.gz\"\n",
    "training_job_name = \"huggingface-finetune-twitter2023-04-21--2023-04-21-02-02-26-020\"\n",
    "s3_model_tar_gz_uri = \"s3://{}/{}/output/{}\".format(sess.default_bucket(),training_job_name,MDDEL_TAR_NAME)\n",
    "print(\"Fine tuned model artifact is located at \", s3_model_tar_gz_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58573d94-5035-45ab-aab2-682eab7597df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "S3Downloader.download(s3_uri=s3_model_tar_gz_uri,local_path=PYTORCH_MODEL_LOCATION)\n",
    "\n",
    "print(\"model downloaded from {} and saved locally at {}\".format(s3_model_tar_gz_uri,PYTORCH_MODEL_LOCATION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53961d29-99ac-43c4-aa09-d91330d6a224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "OUTPUT_MODEL_DIR = \"sagemaker_finetuned_model\"\n",
    "\n",
    "def extraction(tar_dir, tar_file, output_dir):\n",
    "    tar_location = os.path.join(tar_dir, tar_file)\n",
    "    with tarfile.open(tar_location, \"r:gz\") as tar:\n",
    "        tar.extractall(os.path.join(tar_dir, output_dir))\n",
    "        print(\"Extracted to \",os.path.join(tar_dir, output_dir))\n",
    "\n",
    "extraction(PYTORCH_MODEL_LOCATION, MDDEL_TAR_NAME, OUTPUT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10690d-670b-4343-8331-25398b1f47cb",
   "metadata": {},
   "source": [
    "#### We now copy the fine funed Pytorch model to the tokenizer dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c01796-8e99-4b48-86fb-fc3f670eb877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can check the model performance with different check point file\n",
    "check_point = \"checkpoint-5500\"\n",
    "PYTORCH_BIN_LOCATION = os.path.join(PYTORCH_MODEL_LOCATION,OUTPUT_MODEL_DIR,check_point,\"pytorch_model.bin\")\n",
    "PYTORCH_BIN_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f50d49-f7c5-412d-b017-51c57fafd2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tar_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253820ee-ccb5-43fd-9e5c-393ecb7b7635",
   "metadata": {},
   "source": [
    "#### Replace the pytorch_model.bin with fine tuned model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d4d45-edcf-4479-85f2-aea4b83bbb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.file_util import copy_file\n",
    "copy_file(PYTORCH_BIN_LOCATION, str(model_tar_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4058689-be44-4c12-9886-70b13734117b",
   "metadata": {},
   "source": [
    "#### Let's test our fine tuned model locally to make sure everything work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdebb12-e6e7-41a7-8d20-c35a52377e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load model and processor from model_dir\n",
    "    model =  AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    return model.to(device), tokenizer\n",
    "\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # unpack model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # process input\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # preprocess\n",
    "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # pass inputs with all kwargs in data\n",
    "        if parameters is not None:\n",
    "            outputs = model.generate(input_ids, **parameters)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids)\n",
    "\n",
    "    # postprocess the prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return [{\"generated_text\": prediction}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63f699-ba07-4282-ac22-ca1221057b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_and_tokenizer = model_fn(model_tar_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a37373-e26e-4e60-a162-4c79a378fa92",
   "metadata": {},
   "source": [
    "#### Let's test our fine tuned model summerization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c4cb8-b694-4ccf-9b50-c3860538df4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"summarize: Lenny: Babe, can you help me with something?\\r\\nBob: Sure, what's up?\\r\\nLenny: Which one should I pick?\\r\\nBob: Send me photos\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nBob: I like the first ones best\\r\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\r\\nBob: I have four black pairs :D :D\\r\\nLenny: yeah, but shouldn't I pick a different color?\\r\\nBob: what matters is what you'll give you the most outfit options\\r\\nLenny: So I guess I'll buy the first or the third pair then\\r\\nBob: Pick the best quality then\\r\\nLenny: ur right, thx\\r\\nBob: no prob :\"\n",
    "input_json = {\n",
    "    \"inputs\": text\n",
    "}\n",
    "print(f\"{text}\\n---------------\")\n",
    "results = predict_fn(input_json,model_and_tokenizer)\n",
    "print(f\"{results}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286b738-7d49-4bab-bf83-00401471f896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/inference_flan_t5_model_hub.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c175ae-3889-4d42-a430-d35fadbe9c53",
   "metadata": {},
   "source": [
    "#### Let's copy our inference code alongside the model artifact and prepare to deploy on SageMaker Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6435ac-3b8c-475f-a225-187b168e3ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "inference_code_dir = Path(model_tar_dir,\"code\")\n",
    "if not path.exists(inference_code_dir):\n",
    "    inference_code_dir.mkdir()\n",
    "copy_file(src=\"./scripts/inference_flan_t5_model_hub.py\",dst=path.join(str(inference_code_dir),\"inference.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c0b9d-449e-4bc7-90ef-48c0cc646885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1fa9cd-2cc4-4308-b47e-43c9cb4dd5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/flan-t5-small\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14116ccb-804c-4b40-9f83-dedfdf966a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#s3_model_uri = \"s3://sagemaker-eu-west-1-707684582322/flan-t5-small/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92419feb-7e5d-4c74-bc63-2d4b0bc6d245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.17\",  # transformers version used\n",
    "   pytorch_version=\"1.10\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7bee0a-f51e-4370-9872-8e18d9054940",
   "metadata": {},
   "source": [
    "#### Run inference using the deployed model with python sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283404b2-95be-4790-80c0-4a17f54a74f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebaf843-6733-4b2f-98d6-c61cf62932f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"sagemaker-runtime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa2b4a-6285-4e73-bcae-9882ed5b2505",
   "metadata": {},
   "source": [
    "**Put the correct endpoint name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b727e8f-14a9-448f-aada-9b574ffa52a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"huggingface-pytorch-inference-2023-05-03-02-47-34-073\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20537f-6cc9-4b7f-9bbb-04eba515acba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Sentiment classification: The house is wonderful\"\n",
    "text = \"summarize: Lenny: Babe, can you help me with something?\\r\\nBob: Sure, what's up?\\r\\nLenny: Which one should I pick?\\r\\nBob: Send me photos\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nLenny:  <file_photo>\\r\\nBob: I like the first ones best\\r\\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\\r\\nBob: I have four black pairs :D :D\\r\\nLenny: yeah, but shouldn't I pick a different color?\\r\\nBob: what matters is what you'll give you the most outfit options\\r\\nLenny: So I guess I'll buy the first or the third pair then\\r\\nBob: Pick the best quality then\\r\\nLenny: ur right, thx\\r\\nBob: no prob :\"\n",
    "input_json = {\n",
    "    \"inputs\": text\n",
    "}\n",
    "print(f\"{text}\\n---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dde1e1-5b3c-40fe-a954-89e94b7f3c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(input_json),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49867dd0-e3bc-4eca-a101-37f3f3c8043a",
   "metadata": {},
   "source": [
    "#### Don't forget to delete your endpoint once finished testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5daa014-565d-4b04-bd7d-54ae8ff9c24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
